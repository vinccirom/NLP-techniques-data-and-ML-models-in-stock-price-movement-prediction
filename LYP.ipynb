{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST YEAR PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "#!pip install pickle5\n",
    "#import pickle5 as pickle # windows python 3.7.6 not supporting Protocol 5\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# for processing\n",
    "import re\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import feature_selection, feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# for bag-of-words\n",
    "\n",
    "# for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Twitter Scraper library\n",
    "#!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "#!pip install flair\n",
    "import flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Class (**Data**) is responsible for loading the data (Companies with their corresponding 8k reports) in an interpretable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    '''\n",
    "    This class loads, and organises the data in a way that it can be manipulated in a desired manner.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._8k_reports = {}\n",
    "        self._8k_r_path = 'data/8K-gz/'\n",
    "        self.load_data()\n",
    "        self.separate_reports()\n",
    "    \n",
    "    def load_data(self):\n",
    "        '''\n",
    "        Fill dictionary with company symbols as keys and their respective reports (byte objects) as values.\n",
    "        '''\n",
    "        #if os.path.isfile('data/separate_reports.pickle'):\n",
    "            #with open('data/separate_reports.pickle', 'rb') as handle:\n",
    "                #self._8k_reports = pickle.load(handle)\n",
    "        #else:\n",
    "        symbols = [f for f in os.listdir(self._8k_r_path) if not f.startswith('.')]\n",
    "        #symbols = [s.replace('._', '') for s in symbols] #Only when using windows\n",
    "        for symbol in symbols:\n",
    "            directory = self._8k_r_path + symbol\n",
    "            #print(directory, symbol)\n",
    "            #print(symbol.replace('.gz',''))\n",
    "            with gzip.open(directory, 'rb') as f:\n",
    "                self._8k_reports[symbol.replace('.gz','')] = f.read()\n",
    "            \n",
    "            #with open('data/raw_data.pickle', 'wb') as handle:\n",
    "                #pickle.dump(self._8k_reports, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    def separate_reports(self):\n",
    "        '''\n",
    "        Replace the dictionary values from byte object reports to a list of unique/separate string reports.\n",
    "        '''\n",
    "        #if os.path.isfile('data/separate_reports.pickle') == False:\n",
    "        companies = []\n",
    "        report_list = []\n",
    "        for (symbol, reports) in self._8k_reports.items():\n",
    "            # Separate reports by the '</DOCUMENT>' delimiter and remove the last item of the list (\\\\n').\n",
    "            reports_list = str(reports).split('</DOCUMENT>')\n",
    "            reports_list.pop(-1)\n",
    "            self._8k_reports[symbol] = reports_list\n",
    "            for report in reports_list:\n",
    "                companies.append(symbol)\n",
    "                report_list.append(report)\n",
    "        self.dataframe(companies, report_list)    \n",
    "                \n",
    "            #with open('data/separate_reports.pickle', 'wb') as handle:\n",
    "                #pickle.dump(self._8k_reports, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    def dataframe(self, companies, reports):        \n",
    "        self.df_data = pd.DataFrame({'company':companies, 'report': reports})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting number of 8-k reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of 8k reports: ', len(data.df_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8k report categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New classification\n",
    "report_categories = ['Entry into a Material Definitive Agreement',\n",
    "'Termination of a Material Definitive Agreement',\n",
    "'Mine Safety - Reporting of Shutdowns and Patterns of Violations',\n",
    "#----------\n",
    "'Completion of Acquisition or Disposition of Assets',\n",
    "'Results of Operations and Financial Condition',\n",
    "'Creation of a Direct Financial Obligation or an Obligation under an Off-Balance Sheet Arrangement of a Registrant',\n",
    "'Triggering Events That Accelerate or Increase a Direct Financial Obligation or an Obligation under an Off-Balance Sheet Arrangement',\n",
    "'Costs Associated with Exit or Disposal Activities',\n",
    "'Material Impairments',\n",
    "#----------\n",
    "'Notice of Delisting or Failure to Satisfy a Continued Listing Rule or Standard; Transfer of Listing',\n",
    "'Unregistered Sales of Equity Securities',\n",
    "'Material Modification to Rights of Security Holders',\n",
    "#----------\n",
    "'Non-Reliance on Previously Issued Financial Statements or a Related Audit Report or Completed Interim Review',\n",
    "#----------\n",
    "'Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers',\n",
    "'Departure of Directors or Principal Officers; Election of Directors; Appointment of Principal Officers',\n",
    "'Amendments to Articles of Incorporation or Bylaws; Change in Fiscal Year',\n",
    "'Amendment to Registrant\\\\\\'s Code of Ethics, or Waiver of a Provision of the Code of Ethics',\n",
    "'Change in Shell Company Status',\n",
    "'Submission of Matters to a Vote of Security Holders',\n",
    "'Shareholder Director Nominations',\n",
    "#-----------\n",
    "'ABS Informational and Computational Material',\n",
    "'Change of Servicer or Trustee',\n",
    "'Change in Credit Enhancement or Other External Support',\n",
    "'Failure to Make a Required Distribution',\n",
    "'Securities Act Updating Disclosure',\n",
    "#-----------\n",
    "'Regulation FD Disclosure',\n",
    "#-----------\n",
    "'Other Events (The registrant can use this Item to report events that are not specifically called for by Form 8-K, that the registrant considers to be of importance to security holders.)',\n",
    "#-----------\n",
    "'Financial Statements and Exhibits', \n",
    "# Old categorisation\n",
    "'Changes in Control of Registrant',\n",
    "'Acquisition or Disposition of Assets',\n",
    "'Bankruptcy or Receivership',\n",
    "'Changes in Registrant\\\\\\'s Certifying Accountant',\n",
    "'Other Events',\n",
    "'Other events',\n",
    "'Resignation of Registrant\\\\\\'s Directors',\n",
    "'Change in Fiscal Year',\n",
    "'Amendments to the Registrant\\\\\\'s Code of Ethics',\n",
    "'Temporary Suspension of Trading Under Registrant\\\\\\'s Employee Benefit Plans']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature preprocessing (without Non-negative matrix factorisation)\n",
    "\n",
    "**Non-linguistic features (3):**\n",
    "1. Normalised price changes (UP: 1, DOWN: -1, STAY: 0)\n",
    "2. Moving averages\n",
    "3. Volatility index\n",
    "- Earnings surprise - **Not used because it limits the ability to comprehend how effective the NLP techniques and ML models are to predict since this feature is highly informative of the price direction.** We want to be able to predict price movements of any type of reports and not just those ones mentioning earnings surprise.\n",
    "\n",
    "**Linguistic features:**\n",
    "- Event category (the reason for filing the form 8-K)\n",
    "- Reports Polarity score (Positive or Negative)\n",
    "- EXPLAIN BETTER - TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import historical prices of S&P500 as there is data found all the way from 2002 (Unlike S&P1500 which is only found from the year 2008). This index still provides a good notion of how the market is performing overall, thus can be used to normalise stock prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    '''\n",
    "    This class...\n",
    "    '''\n",
    "    def __init__(self, data, report_categories):\n",
    "        self.data = data # raw data\n",
    "        self.sp500_hist_prices = pd.read_csv(\"data/sp500_prices.csv\") # S&P 500 Historical prices\n",
    "        self.VIX = pd.read_csv(\"data/VIX.csv\").sort_values(['Date'], ascending=False) # S&P 500 volatility prices\n",
    "        self.lexicons_dic = {}\n",
    "        self.sentiment_lexicons() # Load csv file containing Financial words scored and add positive and negative words to lexicons_dic\n",
    "        self.reports_polarity_score = []\n",
    "        self.preprocessed_reports = [] # All the reports in a clean format\n",
    "        self.tfidf = [] # TF-IDF values of all reports\n",
    "        self.report_categories = report_categories # all possible categories a report can be classified as\n",
    "        self.df_dates_categories = pd.DataFrame(columns=['company','report date','event category'])\n",
    "        self.normalised_price_change = {} # Normalised companies' price changes after 8k doc release\n",
    "        self.recent_movements = {} # Companies recent price movements (1 week, monthly, quartely and yearly)\n",
    "        self.volatility = {} # Index volatily before a report is submitted   \n",
    "        self.negation_heuristic = False\n",
    "        self.linguistic_models = {}\n",
    "        self.df_prep_data = pd.DataFrame(columns=['company','preprocessed report','date','report polarity','categories','week move','month move','quarter move','year move','volatility','price direction'])\n",
    "\n",
    "        \n",
    "    def reports_date_categories(self):\n",
    "        '''\n",
    "        Perform a general analysis of the 8k reports from a given company and return the correspondent date stamps\n",
    "        of each report, along with the event categories of each.\n",
    "        '''\n",
    "        date_stamp = None\n",
    "        event_categories = []\n",
    "        for index, row in self.data.iterrows():\n",
    "            dic = {}\n",
    "            report = row['report']\n",
    "            company = row['company']\n",
    "            match = re.search(r\"TIME:(\\d+)\\\\\", report) # Regex expression --> TIME:YYYMMDDHHMMSS\\\\\n",
    "            match = report[match.span()[0]+5:match.span()[1]-7] # Only keep --> YYYYMMDD\n",
    "            file_path = 'data/price_history/' + company + '.csv' # Load the csv named after the parameter company to extract prices\n",
    "            company_prices = pd.read_csv(file_path) # Load the company's historical prices\n",
    "            temp_date = datetime.strptime(match, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "            if temp_date in company_prices.Date.values:\n",
    "                dic['company']=company\n",
    "                dic['date']=temp_date\n",
    "                categ = [] # List of categories for each report\n",
    "                for category in self.report_categories:\n",
    "                    categ_match = re.search(category, report)\n",
    "                    if categ_match is not None:\n",
    "                        categ.append(categ_match.group())\n",
    "                if len(categ) == 0:\n",
    "                    dic['categories']=['Unknown']\n",
    "                else:\n",
    "                    dic['categories']=categ\n",
    "            else: \n",
    "                dic['company']=company\n",
    "                dic['categories']=['Unknown']\n",
    "                dic['date']=None\n",
    "            self.df_dates_categories = self.df_dates_categories.append(dic, ignore_index=True)\n",
    "  \n",
    "    \n",
    "    def load_file(self, file_name, file_format = '.pickle'):\n",
    "        print('Loading file:', file_name, file_format, '...')\n",
    "        file_path = 'data/' + file_name + file_format\n",
    "        with open(file_path, 'rb') as handle:\n",
    "            file = pickle.load(handle)\n",
    "        return file\n",
    "    \n",
    "    def save_file(self, file_name, saving_var, file_format = '.pickle'):\n",
    "        print('Saving file:', file_name, file_format, '...')\n",
    "        file_path = 'data/' + file_name + file_format\n",
    "        with open(file_path, 'wb') as handle:\n",
    "            pickle.dump(saving_var, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "            \n",
    "        \n",
    "    def sentiment_lexicons(self):\n",
    "        '''\n",
    "        Custom dictionary of negative and positive words specific to the accounting and financial domain obtained\n",
    "        from the Loughran and McDonald (2011) article.\n",
    "        SENTIMENT LEXICONS (POLARITY: NEGATIVE OR POSITIVE)\n",
    "        '''\n",
    "        # These sentiment lexicons lack a scoring measurement \n",
    "        lexicons = pd.read_csv('data/LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "        lexicons = lexicons[['Word','Negative','Positive']]\n",
    "        lexicons.drop(lexicons[(lexicons.Negative == 0) & (lexicons.Positive == 0)].index, inplace=True)\n",
    "        neg, pos = [], []\n",
    "        for index, row in lexicons.iterrows():\n",
    "            if row['Negative'] != 0:\n",
    "                neg.append(row['Word'].lower())\n",
    "            else:\n",
    "                pos.append(row['Word'].lower())\n",
    "        self.lexicons_dic['Positive'] = pos\n",
    "        self.lexicons_dic['Negative'] = neg\n",
    "    \n",
    "    def sentiment_score(self, token_list):\n",
    "        '''\n",
    "        This function uses the Loughran and McDonald list of financial domain-specific words classified as positive\n",
    "        or negative. However, due to the scarce number of pos (354) and neg (354) words, this function also obtains\n",
    "        sentiment scores from the SentiWordNet if a given word is not found of the previous dictionary. In addition,\n",
    "        the function takes into account words that have been negated by giving them an opposite scored to the one obtained (if any).\n",
    "        Negative reports will generally have a greater tendency to be well represented by sentiment scores due to\n",
    "        the wider list of domain-specific words available.\n",
    "        '''\n",
    "        score = 0\n",
    "        for token in token_list:\n",
    "            found_LM = False # Indicative of a word found in the financial sentiment lexicon dictionary\n",
    "            negated_word = False\n",
    "            if '_NEG' in token:\n",
    "                token = re.sub('_NEG', '',token) # remove the '_NEG' tag to find the score of the actual word\n",
    "                negated_word = True\n",
    "            if token in self.lexicons_dic['Positive']:\n",
    "                score += 0.5\n",
    "                found_LM = True\n",
    "            elif token in self.lexicons_dic['Negative']:\n",
    "                score -= 0.5\n",
    "                found_LM = True        \n",
    "            if not found_LM and wn.synsets(token): # if the word is not found in LM dictionary, use SentiWordNet to get a score (if any)\n",
    "                breakdown = swn.senti_synset(wn.synsets(token)[0].name())\n",
    "                if breakdown.pos_score() > 0.5 and breakdown.neg_score() < 0.4:\n",
    "                    score += 0.5\n",
    "                elif breakdown.neg_score() > 0.5 and breakdown.pos_score() < 0.4:\n",
    "                    score -= 0.5\n",
    "            if negated_word:\n",
    "                score = score * -1 # Provide negated score for negated words\n",
    "        return score\n",
    "\n",
    "                    \n",
    "    def reports_polarity(self):\n",
    "        '''\n",
    "        Flattened list of the polarity of all the reports available.\n",
    "        '''\n",
    "        for index, row in self.data.iterrows():\n",
    "            report = row['report']\n",
    "            report_tks = self.report_tokens(report)\n",
    "            self.reports_polarity_score.append(self.sentiment_score(report_tks))\n",
    "            self.preprocessed_reports.append(report_tks) # add tokenized report to list, so after it can be used for get TF-IDF and VH values.\n",
    "\n",
    "            \n",
    "    def report_tokens(self, report):\n",
    "        '''\n",
    "        Incorporate a model of negation by marking as a negative every word appearing between a linguistic negation and a clause-level punctuation mark.\n",
    "        All words are lemmatized. Removing special characters, and numbers, as well as making the entire report small case for processing purposes.\n",
    "        '''\n",
    "        report = re.sub('\\\\\\\\n', ' ', report)\n",
    "        report = re.sub('\\\\\\\\t', ' ', report)\n",
    "        report = re.sub(\"\\\\\\\\'\", \"'\", report)\n",
    "        report = report.lower()\n",
    "\n",
    "        if self.negation_heuristic:\n",
    "            # Negation heuristic\n",
    "            temp_report = re.findall('(?:never|no |nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)(.*?)[^\\w\\s]', report)\n",
    "            # list of negated tokens (e.g. win_NEG) - to later add to unigram\n",
    "            negated_tokens = [tk+'_NEG' for tr in temp_report for tk in word_tokenize(tr)]            \n",
    "            # new report without the linguistic negations and the original negated tokens\n",
    "            report = re.sub('(?:never|no |nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)(.*?)[^\\w\\s]', '', report)\n",
    "            temp_tokens = word_tokenize(report) + negated_tokens\n",
    "        else:\n",
    "            temp_tokens = word_tokenize(report)\n",
    "            \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        stop = stopwords.words('english') # Remove English stop words\n",
    "        tokens = []\n",
    "        for tk in temp_tokens:\n",
    "            lemmatized_tk = lemmatizer.lemmatize(tk)\n",
    "            if lemmatized_tk.isalpha() and lemmatized_tk not in stop and wn.synsets(lemmatized_tk) or '_NEG' in lemmatized_tk:\n",
    "                tokens.append(lemmatized_tk)\n",
    "                \n",
    "        return tokens\n",
    "\n",
    "        \n",
    "    def compute_others(self):\n",
    "        '''\n",
    "        Compute stock price, stock avg movements and volatility, price direction (up,stay,down) and add them \n",
    "        to a pandas dataframe plus the preprocessed reports, dates and categories.\n",
    "        '''\n",
    "        if os.path.isfile('data/df_prep_data.pkl'):\n",
    "        # if directory has already been created, load pickle files into variables\n",
    "            self.df_prep_data = pd.read_pickle(\"data/df_prep_data.pkl\")\n",
    "            self.df_dates_categories = pd.read_pickle(\"data/reports_date_cat.pkl\")\n",
    "            self.reports_polarity_score = self.load_file('reports_polarity')\n",
    "            self.preprocessed_reports = self.load_file('preprocessed_reports')\n",
    "        else:\n",
    "            self.reports_date_categories()\n",
    "            self.reports_polarity()\n",
    "            for i, (index, row) in enumerate(self.df_dates_categories.iterrows()):\n",
    "                dic = {}\n",
    "                company = row['company']\n",
    "                date = row['date']\n",
    "                categories = row['categories']\n",
    "                if date is not None:\n",
    "                    file_path = 'data/price_history/' + company + '.csv' # Load the csv named after the parameter company to extract prices\n",
    "                    company_prices = pd.read_csv(file_path) # Load the company's historical prices\n",
    "                if date in company_prices.Date.values and date in self.sp500_hist_prices.Date.values: # Check if there is s&p500 data and companies' stock price from the date extracted in the 8k report\n",
    "                    dic['company']=company\n",
    "                    dic['preprocessed report']=self.preprocessed_reports[i]\n",
    "                    dic['date']=date\n",
    "                    dic['report polarity']=self.reports_polarity_score[i]\n",
    "                    dic['categories']=categories\n",
    "                    recent_moves = self.MA(company, date)\n",
    "                    dic['week move']=recent_moves[0]\n",
    "                    dic['month move']=recent_moves[1]\n",
    "                    dic['quarter move']=recent_moves[2]\n",
    "                    dic['year move']=recent_moves[3]\n",
    "                    dic['volatility']=self.volatility_index(date)\n",
    "                    dic['price direction']=self.normal_price_change(company, date)   \n",
    "                self.df_prep_data = self.df_prep_data.append(dic, ignore_index=True)\n",
    "            \n",
    "            # Save and update dates and categories dataframe before one-hot-encoding\n",
    "            self.df_dates_categories = self.df_prep_data[['company','categories','date']]\n",
    "            self.df_dates_categories.to_pickle(\"data/reports_date_cat.pkl\")\n",
    "            # One-hot-encode the event categories of each report\n",
    "            df1 = self.df_prep_data['categories'].explode()\n",
    "            self.df_prep_data = self.df_prep_data[self.df_prep_data.columns].join(pd.crosstab(df1.index, df1))\n",
    "            self.df_prep_data.pop('categories')\n",
    "            # Delete all rows containing NaN values\n",
    "            self.df_prep_data.dropna(inplace=True)\n",
    "            # Save and update variables\n",
    "            # New preprocessed reports list required to compute tf-idf values (some were removed as they contained NaN values)\n",
    "            self.preprocessed_reports = self.df_prep_data['preprocessed report'].tolist()\n",
    "            self.save_file(str('preprocessed_reports'), self.preprocessed_reports)\n",
    "            self.reports_polarity_score = self.df_prep_data['report polarity'].tolist()\n",
    "            self.save_file(str('reports_polarity'), self.preprocessed_reports)\n",
    "            # Save Dataframe\n",
    "            self.df_prep_data.to_pickle(\"data/df_prep_data.pkl\")\n",
    "\n",
    "            \n",
    "    def normal_price_change(self, company, close_date):\n",
    "        '''\n",
    "        Compute the difference in the company’s stock price before and 2 days after the report is released. \n",
    "        For example, if the 8-K report is published before market opens, this difference is computed between the\n",
    "        price at the 2 days after open and the price at the release date close. Normalize this difference by subtracting \n",
    "        the same difference computed for the entire S&P 1500 index (stock index GSPC) for the same period.\n",
    "        '''        \n",
    "        file_path = 'data/price_history/' + company + '.csv' # Load the csv named after the parameter company to extract prices\n",
    "        company_prices = pd.read_csv(file_path) # Load the company's historical prices\n",
    "        # Extract the values from columns *Close* and *Open* where the value in the column Date is equal to the variable **date**.\n",
    "        \n",
    "        adjclose_STOCK = company_prices.loc[company_prices[\"Date\"] == close_date, [\"Close\"]].values.item() # Close price\n",
    "        index = company_prices.loc[company_prices[\"Date\"] == close_date].index[0] # index of Next day Open price date\n",
    "        if index+2 < len(company_prices.index): \n",
    "            next2days_open_STOCK = company_prices.iloc[index+2][\"Open\"] # 2 days after Open price date\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        diff_STOCK = next2days_open_STOCK - adjclose_STOCK # Calculate difference in STOCK price\n",
    "        perc_change_STOCK = (abs(diff_STOCK)/adjclose_STOCK) * 100 # Calculate percentage change in STOCK price\n",
    "        if diff_STOCK < 0: # Account for positive or negative percentage change in STOCK price\n",
    "            perc_change_STOCK = perc_change_STOCK*-1 \n",
    "\n",
    "        adjclose_SP500 = self.sp500_hist_prices.loc[self.sp500_hist_prices[\"Date\"] == close_date, [\"Close\"]].values.item()\n",
    "        index = self.sp500_hist_prices.loc[self.sp500_hist_prices[\"Date\"] == close_date].index[0]                                            \n",
    "        next2days_open_SP500 = self.sp500_hist_prices.iloc[index+2][\"Open\"]\n",
    "\n",
    "        diff_SP500 = next2days_open_SP500 - adjclose_SP500\n",
    "        perc_change_SP500 = (abs(diff_SP500)/adjclose_SP500) * 100\n",
    "        if diff_SP500 < 0:\n",
    "            perc_change_SP500 = perc_change_SP500*-1\n",
    "        \n",
    "        price_change_normalised = perc_change_STOCK - perc_change_SP500 # Sum/Subtract (Normalise) the stock price change with the SP500 INDEX price change\n",
    "        if price_change_normalised > 1:\n",
    "            return 1\n",
    "        elif price_change_normalised < -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def MA(self, company, close_date):\n",
    "        '''\n",
    "        Compute stock average movements (MA - moving average) 4 recent movements features per company: \n",
    "        1 week, 1 month, 1 quarter, and 1 year recent change in price until the event occurs. \n",
    "        To calculate recent price changes, we used (a) a 5-day moving average (MA) for the 1-month price\n",
    "        change feature, (b) 10-day MA for the 1-quarter change, and (c) 20-day MA for 1 year.\n",
    "        We normalized all these change features using the S&P 500 index.\n",
    "        '''        \n",
    "        # Load the csv named after the parameter company to extract prices\n",
    "        file_path = 'data/price_history/' + company + '.csv'\n",
    "        company_prices = pd.read_csv(file_path) # Load the company's historical prices    \n",
    "        SP500_prices = pd.read_csv(\"data/sp500_prices.csv\")\n",
    "        adjclose_STOCK = company_prices.loc[company_prices[\"Date\"] == close_date, [\"Close\"]].values.item()\n",
    "        adjclose_SP500 = SP500_prices.loc[SP500_prices[\"Date\"] == close_date, [\"Close\"]].values.item() \n",
    "        start_index = company_prices.loc[company_prices[\"Date\"] == close_date].index[0]\n",
    "        SP500_index = SP500_prices.loc[SP500_prices[\"Date\"] == close_date].index[0]\n",
    "\n",
    "        recent_movements = []\n",
    "\n",
    "        ####################### 1 WEEK (7 DAYS) PRICE CHANGE ##############################\n",
    "        if start_index + 7 < len(company_prices.index):\n",
    "            diff_STOCK = adjclose_STOCK - company_prices.iloc[start_index+7]['Close']\n",
    "            perc_change_STOCK = (abs(diff_STOCK)/adjclose_STOCK) * 100 # Calculate percentage change in STOCK price\n",
    "            if diff_STOCK < 0: # Account for positive or negative percentage change in STOCK price\n",
    "                perc_change_STOCK = perc_change_STOCK*-1\n",
    "            recent_movements.append(perc_change_STOCK)\n",
    "        else:\n",
    "            recent_movements.append(0) # If there is no data available 7 days before, we'll assume that the price has remained constant, thus 0% change\n",
    "\n",
    "        #1 MONTH (21 trading days), 1 QUARTER (63 trading days), 1 YEAR (253 trading days) PRICE CHANGE (USING MOVING AVERAGES) \n",
    "        intervals = [(21, 5), (63,10), (253,20)]\n",
    "        for interval in intervals:\n",
    "            if start_index + interval[0] + interval[1] >= len(company_prices.index): # Check there is data available for the previous dates (1months, 1quarter and 1year earlier)\n",
    "                stock_prices = []\n",
    "                SP500_prices = []    \n",
    "                # Start (current index), Stop (current index + X trading days), Step (index + MA days, e.g. 5, 10, 20)\n",
    "                for index in range(start_index, start_index+interval[0], start_index+interval[0]+interval[1]):\n",
    "                    stock_prices.append(company_prices.iloc[index]['Close'])\n",
    "                for index in range(start_index, SP500_index+interval[0], SP500_index+interval[0]+interval[1]):\n",
    "                    SP500_prices.append(self.sp500_hist_prices.iloc[index]['Close'])\n",
    "                avg_stock_price = sum(stock_prices)/len(stock_prices)\n",
    "                avg_SP500_price = sum(SP500_prices)/len(SP500_prices)\n",
    "                diff_STOCK = adjclose_STOCK - avg_stock_price\n",
    "                diff_SP500 = adjclose_SP500 - avg_SP500_price\n",
    "                perc_change_STOCK = (abs(diff_STOCK)/adjclose_STOCK) * 100 # Calculate percentage change in STOCK price\n",
    "                perc_change_SP500 = (abs(diff_SP500)/adjclose_SP500) * 100 # Calculate percentage change in STOCK price\n",
    "                if diff_STOCK < 0: # Account for positive or negative percentage change in STOCK price\n",
    "                    perc_change_STOCK = perc_change_STOCK*-1\n",
    "                if diff_SP500 < 0:\n",
    "                    perc_change_SP500 = perc_change_SP500*-1 \n",
    "                price_change_normalised = perc_change_STOCK - perc_change_SP500 # Sum/Subtract (Normalise) the stock price change with the SP500 INDEX price change\n",
    "                recent_movements.append(price_change_normalised)\n",
    "            else:\n",
    "                # If there is no data available 7 days before, we'll assume that the price has remained constant, thus 0% change\n",
    "                # This assumption should not make a noticeable impact to the model, since the number of 8k docs reported by companies during the first\n",
    "                # year of the dataset (2002) is very little compared to the rest of the dataset.\n",
    "                recent_movements.append(0) \n",
    "\n",
    "        return recent_movements\n",
    "\n",
    "        \n",
    "    def volatility_index(self, date):\n",
    "        '''\n",
    "        Compute a company\"s volatility index at a given date.\n",
    "        '''\n",
    "        index = self.VIX.loc[self.VIX[\"Date\"] == date].index[0]\n",
    "        return self.VIX.iloc[index-1][\"Price\"]\n",
    "\n",
    "        \n",
    "    def features(self, negation_heuristic = False):\n",
    "        '''\n",
    "        Calls all required functions to preprocess all features\n",
    "        '''\n",
    "        self.negation_heuristic = negation_heuristic\n",
    "        self.compute_others()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the TF-IDF values from 8K reports to classify sentiment into negative or positive reports, we instead train these to predict price change (up, stay, down). This decision is based on the fact that 8K documents report major events that shareholders should know about, hence a positive or negative sentiment classification would most of the times result in the stock price going up or down, so we take advantage of this direct relationship between variables and use the tf-idf values to predict changes in one go. However, the predicted change is not used as a feature for the last ensemble model, instead, this one is multiplied by the accuracy of its predicting model in order to account for predictive confidence. It is also convenient to predict price changes instead of polarity (negative or positive) as these can be used as y labels for the supervised classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tf-Idf (advanced variant of BoW)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def tfidf_matrix(X_preprocessed_reports, vectorizer = None, tfidf_indexes_kept = None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',tokenizer=dummy_fun,preprocessor=dummy_fun,token_pattern=None, max_features=40000)\n",
    "        X = vectorizer.fit_transform(X_preprocessed_reports)\n",
    "        dic_vocabulary = vectorizer.vocabulary_\n",
    "        return vectorizer, X\n",
    "    else:\n",
    "        X = vectorizer.transform(X_preprocessed_reports)\n",
    "        X = np.array(X.toarray())\n",
    "        tfidf_indexes_NOT_kept = [index for index in range(len(X[0])) if index not in tfidf_indexes_kept]\n",
    "        X = np.delete(X, tfidf_indexes_NOT_kept, 1)    \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to drop some columns and reduce the matrix dimensionality, we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:\n",
    "\n",
    "1. treat each category as binary (for example, the “UP” category is 1 for the 8K docs that led to the price UP and 0 for the others);\n",
    "2. perform a Chi-Square test to determine whether a feature and the (binary) target are independent;\n",
    "3. keep only the features with a certain p-value from the Chi-Square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_dim_reduction(vectorizer, X_train_matrix, y):\n",
    "    X_names = vectorizer.get_feature_names()\n",
    "    p_value_limit = 0.95\n",
    "    dtf_features = pd.DataFrame()\n",
    "    for cat in np.unique(y):\n",
    "        chi2, p = feature_selection.chi2(X_train_matrix, y==cat)\n",
    "        dtf_features = dtf_features.append(pd.DataFrame({\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "        dtf_features = dtf_features.sort_values([\"y\",\"score\"],ascending=[True,False])\n",
    "        dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "    X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "    print(\"According to the figures used for the Chi Square Test (and p-value), the maximum number of features should be:\", len(X_names))\n",
    "    \n",
    "    for cat in np.unique(y):\n",
    "       print(\"# {}:\".format(cat))\n",
    "       print(\"  . selected features:\",len(dtf_features[dtf_features[\"y\"]==cat]))\n",
    "       print(\"  . top features:\", \",\".join(dtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n",
    "       print(\" \")\n",
    "\n",
    "    # The feature matrix (X_train_matrix) has a shape of 15,144 x 40,000 (# of 8K docs in trainng x max length of vocab) \n",
    "    # sns.heatmap(X_train_matrix.todense()[:,np.random.randint(0,X_train_matrix.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')\n",
    "    #plt.savefig('data/Sparse tf-idf matrix.png')\n",
    "    \n",
    "    # Further dimensionality reduction can be done by deleting the columns representing words that appear less than X times\n",
    "    cols = [None] * len(vectorizer.vocabulary_.items())\n",
    "    for word, i in vectorizer.vocabulary_.items():\n",
    "        cols[i] = word\n",
    "\n",
    "    df = pd.DataFrame(X_train_matrix.toarray(), columns = cols)\n",
    "\n",
    "    print('Num. of features/vocabulary length before: ',len(cols))\n",
    "    df = df.loc[:, df.astype(bool).sum(axis=0)>11] # Keep words that appear more than 11 times in the corpus\n",
    "    print('Num. of features/vocabulary length after selecting words that appear at least 12 times in the corpus:', len(df.columns))\n",
    "    # indexes kept after removing unfrequent words\n",
    "    tfidf_indexes_kept = [vectorizer.vocabulary_[column_name] for column_name in df.columns]\n",
    "    \n",
    "    return tfidf_indexes_kept\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_tfidf(X_train_matrix, train_y, X_test_matrix, test_y):\n",
    "    train_y = train_y.astype('int')\n",
    "    \n",
    "    classifier = naive_bayes.MultinomialNB()\n",
    "    kf = model_selection.KFold(n_splits=5)\n",
    "    NB_kfold_cross_val_accuracy = cross_val_score(classifier, X_train_matrix, train_y, cv = kf).mean()*100\n",
    "    print(\"Naive Bayes Kfold Cross Validation Mean Accuracy Score -> \",NB_kfold_cross_val_accuracy, '%')\n",
    "\n",
    "    ## train classifier\n",
    "    classifier.fit(X_train_matrix, train_y)\n",
    "    \n",
    "    #train\n",
    "    train_predicted = classifier.predict(X_train_matrix)\n",
    "    train_predicted_prob = classifier.predict_proba(X_train_matrix)\n",
    "    \n",
    "    ## test\n",
    "    test_predicted = classifier.predict(X_test_matrix)\n",
    "    test_predicted_prob = classifier.predict_proba(X_test_matrix)\n",
    "\n",
    "    classes = np.unique(test_y)\n",
    "    test_y_array = pd.get_dummies(test_y, drop_first=False).values\n",
    "    ## Accuracy, Precision, Recall\n",
    "    test_y = test_y.astype('int')\n",
    "    test_predicted = test_predicted.astype('int')\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(test_y, test_predicted)\n",
    "    auc = metrics.roc_auc_score(test_y, test_predicted_prob,multi_class=\"ovr\")\n",
    "    print(\"Accuracy:\",  round(accuracy,2))\n",
    "    print(\"Auc:\", round(auc,2))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(test_y, test_predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(test_y, test_predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes,yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_y_array[:,i],test_predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3,label='{0} (area={1:0.2f})'.format(classes[i],metrics.auc(fpr, tpr)))\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],xlabel='False Positive Rate',ylabel=\"True Positive Rate (Recall)\",title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(test_y_array[:,i], test_predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3,label='{0} (area={1:0.2f})'.format(classes[i],metrics.auc(recall, precision)))\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()\n",
    "    return train_predicted, train_predicted_prob,  test_predicted, test_predicted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from using different models for performance comparison purposes, the investigation will also use different NLP strategies to represent report documents in order to evaluate the impact of text representation in the accuracy of the models.\n",
    "\n",
    "\n",
    "- For every report, text features are represented by converting the corpus unigram into numerical values.\n",
    "- For every report, **word embeddings** are created as an overall representation.\n",
    "\n",
    "WHY WE DO NOT CONSIDER ADDING SENTIMENT LEXICON AS AN EXTRA FEATURE:\n",
    "\n",
    "- SentiWordNet (Might not be very representative of the financial vocabulary (and connotations) used in the 8K Forms. Because SeçntiWordNet is open-domain, it does not model well the financial domain. For example, growth has objective or negative sentiment according to SentiWordNet, but it is usually positive in finance.\n",
    "- Finance (Jegadeesh and Wu, 2013). unigram features (after PMI) capture about 77% of sentiment words in the Jegadeesh and Wu lexicon. Second, sentiment lexicons do not contain many of the lexicalized features ranked highly by our model. Among 17 words that were identified as important by our unigram model (increase, growth, new, strong, forward, well, grow, charge, loss, lower, decline, reduce, down, adjust, regulation, offset, reduction), only three words (strong, loss, decline) appear in the Jegadeesh’s sentiment lexicons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS AND DATA STAGES #\n",
    "\n",
    "### Models: ###\n",
    "- RANDOM FOREST - USE BAGGING WITH BOOTSRAPPING\n",
    "- SVM\n",
    "- ANN\n",
    "- NAIVE BAYES\n",
    "\n",
    "\n",
    "Dataset of 8K reports ranging from 2002 to 2012\n",
    "\n",
    "## 2-days after 8k release price change prediction ##\n",
    "\n",
    "4 Architectures and Data Stages. Follow below to see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 8k reports:  20000\n"
     ]
    }
   ],
   "source": [
    "#limited_data = data.df_data.sample(n=20000, random_state=42) # Randomly select a specified (20000) number of rows to keep\n",
    "#print('Total number of 8k reports: ', len(limited_data))\n",
    "#limited_data.to_pickle(\"data/limited_data.pkl\")\n",
    "limited_data = pd.read_pickle(\"data/limited_data.pkl\")\n",
    "print('Total number of 8k reports: ', len(limited_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: reports_polarity .pickle ...\n",
      "Loading file: preprocessed_reports .pickle ...\n"
     ]
    }
   ],
   "source": [
    "# Data Stage 1\n",
    "prep_data = Preprocess(limited_data, report_categories)\n",
    "prep_data.features(negation_heuristic = True)\n",
    "features = prep_data.df_prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows:', features.shape[0], 'Columns:', features.shape[1])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the composition of the dataset, I am going to look into the univariate distribution of the target by showing labels frequency with a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"y\", fontsize=12)\n",
    "features[\"price direction\"].reset_index().groupby(\"price direction\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.savefig('data/Plot of Data Balance-Imbalance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset is relatively balanced as the 8K reports in the entire dataset used resulted in a  similar distributed percent of prices going up, stayed or down. Data Science consideration, as an imbalanced dataset could make it harder for the models to classify the features as a given class that appears noticeably less times during training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training data: 15144 -- # Training labels: 15144 -- # Testing data: 3786 -- # Testing labels 3786\n"
     ]
    }
   ],
   "source": [
    "X = features.loc[:, features.columns != 'price direction']\n",
    "y = features.loc[:, features.columns == 'price direction']\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "print(f'# Training data: {len(train_X)} -- # Training labels: {len(train_y)} -- # Testing data: {len(test_X)} -- # Testing labels {len(test_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows:', X.shape[0], 'Columns:', X.shape[1])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"y\", fontsize=12)\n",
    "train_y[\"price direction\"].reset_index().groupby(\"price direction\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.savefig('data/Balanced Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"y\", fontsize=12)\n",
    "test_y[\"price direction\"].reset_index().groupby(\"price direction\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.savefig('data/Balanced Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_preprocessed_reports = train_X['preprocessed report'].values\n",
    "test_X_preprocessed_reports = test_X['preprocessed report'].values\n",
    "train__y = train_y[\"price direction\"].values\n",
    "test__y = test_y[\"price direction\"].values\n",
    "\n",
    "# Create matrix\n",
    "vectorizer, X_train_matrix = tfidf_matrix(train_X_preprocessed_reports)\n",
    "# Visualize top important words (number of features to keep) using Chi Square Test & Return indexes (features) to keep\n",
    "# after deleting less frequent words (appear less than 12 times)\n",
    "tfidf_indexes_kept = tfidf_dim_reduction(vectorizer, X_train_matrix, train__y)\n",
    "# Get an updated matrix after dimensionality reduction\n",
    "X_train_matrix = tfidf_matrix(train_X_preprocessed_reports, vectorizer, tfidf_indexes_kept)\n",
    "X_test_matrix = tfidf_matrix(test_X_preprocessed_reports, vectorizer, tfidf_indexes_kept)\n",
    "# training Naive Bayes with tf-idf features and price change as values\n",
    "train_predicted, train_predicted_prob, test_predicted, test_predicted_prob = train_test_tfidf(X_train_matrix, train__y, X_test_matrix, test__y)\n",
    "\n",
    "# Normalizing tfidf predicted values (used as features for next models) by multiplying with probability score of the selected class\n",
    "########################################\n",
    "train_tfidf_final = []\n",
    "for index, prediction in enumerate(train_predicted):\n",
    "    train_tfidf_final.append(prediction*max(train_predicted_prob[index]))\n",
    "    \n",
    "test_tfidf_final = []\n",
    "for index, prediction in enumerate(test_predicted):\n",
    "    test_tfidf_final.append(prediction*max(test_predicted_prob[index]))\n",
    "    \n",
    "# Add to training and testing data to keep it organised\n",
    "train_X['tfidf pred. proba'] = train_tfidf_final\n",
    "test_X['tfidf pred. proba'] = test_tfidf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime # import here and not at the top of the notebook to avoid issues with the 'time' import\n",
    "# for cleaning tweets - preprocessing\n",
    "#!pip install wordsegment\n",
    "#!pip install autocorrect\n",
    "#from wordsegment import load, segment\n",
    "#load()\n",
    "#from autocorrect import spell\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentiment_model = flair.models.TextClassifier.load('en-sentiment')\n",
    "\n",
    "def get_tweets(X_data):\n",
    "    report_tweets = []\n",
    "    for idx, (index, row) in enumerate(X_data.iterrows()):\n",
    "        company = row['company']\n",
    "        date = row['date']\n",
    "        # if the release date of the 8K report is older than 2009, then it is ignored (no twitter scraping is done)\n",
    "        if datetime.datetime.strptime(date, '%Y-%m-%d') < datetime.datetime(2009,1,1):\n",
    "            report_tweets.append(None)\n",
    "        else:\n",
    "            next_date = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "            next_date += datetime.timedelta(days=1)\n",
    "            # Use Company ticker and the word 'stock' as search keywords\n",
    "            search_params = f'{company} stock since:{date} lang:en until:{str(next_date)}'\n",
    "\n",
    "            # Creating list to append tweet data to\n",
    "            tweets_list = []\n",
    "            # Using TwitterSearchScraper to scrape data and append tweets to list \n",
    "            for i,tweet in enumerate(sntwitter.TwitterSearchScraper(search_params).get_items()):\n",
    "                # Collect a maximum of 50 tweets per report\n",
    "                if i>50:\n",
    "                    break\n",
    "                # Clean tweets    \n",
    "                clean_tweet = cleaning_tweet(tweet.content)\n",
    "                tweets_list.append([clean_tweet])\n",
    "            report_tweets.append(tweets_list)\n",
    "            if len(tweets_list) > 0:\n",
    "                print(len(tweets_list), 'tweets found for report #', idx)\n",
    "                print(tweets_list[:3])\n",
    "                print(\"\")\n",
    "\n",
    "    return report_tweets\n",
    "    \n",
    "\n",
    "def cleaning_tweet(tweet):\n",
    "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', tweet, flags=re.MULTILINE) # to remove links that start with HTTP/HTTPS in the tweet\n",
    "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', tweet, flags=re.MULTILINE) # to remove other url links\n",
    "    tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\"\",tweet).split())       \n",
    "    #tweet = ' '.join(segment(tweet))\n",
    "    #tweet = ' '.join([spell(w) for w in tweet.split()])\n",
    "    tweet = re.sub(r\"\\d\", \"\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet_nlp = nlp(tweet)\n",
    "    tweet = ' '.join([str(i) for i in tweet_nlp if not i.is_digit]) \n",
    "    tweet\n",
    "\n",
    "    return tweet\n",
    "    \n",
    "\n",
    "def tweet_polarity(clean_tweets):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    ratios = []\n",
    "    for i, group_tweets in enumerate(clean_tweets):\n",
    "        print('report', i)\n",
    "        pos_count = []\n",
    "        neg_count = []\n",
    "        # If no tweets were collected for a given report, add None values to the following variables (afterwards all null\n",
    "        # rows will be deleted\n",
    "        if group_tweets is None or len(group_tweets) < 3:\n",
    "            positives.append(None)\n",
    "            negatives.append(None)\n",
    "            ratios.append(None)\n",
    "        else:\n",
    "            for tweet in group_tweets:\n",
    "                # Add positive count of polarities of tweets only if the confidence score is over 0.60\n",
    "                # Add negative count of polarities of tweets only if the confidence score is over 0.60\n",
    "                tweet = flair.data.Sentence(tweet)\n",
    "                sentiment_model.predict(tweet)\n",
    "                probability = tweet.labels[0].score  # numerical value 0-1\n",
    "                sentiment = tweet.labels[0].value  # 'POSITIVE' or 'NEGATIVE'\n",
    "                if sentiment == 'POSITIVE' and probability >= 0.60:\n",
    "                    pos_count.append(1)\n",
    "                if sentiment == 'NEGATIVE' and probability >= 0.60:\n",
    "                    neg_count.append(1)\n",
    "\n",
    "            pos = sum(pos_count)\n",
    "            neg = sum(neg_count)\n",
    "            # Give a 1 to the polarity with the highest count and 0 to the lowest\n",
    "            if pos > neg:  \n",
    "                positives.append(1)   \n",
    "                negatives.append(0)\n",
    "                # Indicate ratio of winning polarity to all polarities counted    \n",
    "                ratios.append(max(pos,neg)/(pos+neg))\n",
    "            elif pos < neg:\n",
    "                positives.append(0)   \n",
    "                negatives.append(1)\n",
    "                # Indicate ratio of winning polarity to all polarities counted    \n",
    "                ratios.append(max(pos,neg)/(pos+neg))\n",
    "            else:\n",
    "                positives.append(0)   \n",
    "                negatives.append(0)\n",
    "                ratios.append(0)\n",
    "            \n",
    "    return positives, negatives, ratios\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = get_tweets(train_X)\n",
    "positive_scores, negative_scores, ratio_tweets = tweet_polarity(training_tweets)\n",
    "\n",
    "train_X['Aggr. Positive tweet'] = positive_scores\n",
    "train_X['Aggr. Negative tweet'] = negative_scores\n",
    "train_X['ratio tweet'] = ratio_tweets\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "testing_tweets = get_tweets(test_X)\n",
    "positive_scores, negative_scores, ratio_tweets = tweet_polarity(testing_tweets)\n",
    "\n",
    "test_X['Aggr. Positive tweet'] = positive_scores\n",
    "test_X['Aggr. Negative tweet'] = negative_scores\n",
    "test_X['ratio tweet'] = ratio_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model - Stacking Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "\n",
    "# compare ensemble to each baseline classifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "\t# define the base models\n",
    "\tlevel0 = list()\n",
    "\tlevel0.append(('lr', LogisticRegression()))\n",
    "\tlevel0.append(('knn', KNeighborsClassifier()))\n",
    "\tlevel0.append(('cart', DecisionTreeClassifier()))\n",
    "\tlevel0.append(('svm', SVC()))\n",
    "\tlevel0.append(('bayes', GaussianNB()))\n",
    "\t# define meta learner model\n",
    "\tlevel1 = LogisticRegression()\n",
    "\t# define the stacking ensemble\n",
    "\tmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\treturn model\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tmodels['lr'] = LogisticRegression()\n",
    "\tmodels['knn'] = KNeighborsClassifier()\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['svm'] = SVC()\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\tmodels['stacking'] = get_stacking()\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "def run_final_model(X, y, stage_number):\n",
    "    # get the models to evaluate\n",
    "    models = get_models()\n",
    "    # evaluate the models and store results\n",
    "    results, names = list(), list()\n",
    "    for name, model in models.items():\n",
    "        scores = evaluate_model(model, X, y)\n",
    "        results.append(scores)\n",
    "        names.append(name)\n",
    "        print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "    # plot model performance for comparison\n",
    "    pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "    pyplot.savefig(f'data/Training Box Plot Stage {stage_number}')\n",
    "    pyplot.show()\n",
    "    return models\n",
    "\n",
    "\n",
    "def test_data_metrics(test_y, test_predicted, stage_number):\n",
    "    print('----------------------------------------------------')\n",
    "    print('METRICS FOR TESTING DATA')\n",
    "    accuracy = metrics.accuracy_score(test_y, test_predicted)\n",
    "    print(\"Accuracy:\",  round(accuracy,2))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(test_y, test_predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(test_y, test_predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,cbar=False)\n",
    "    classes = np.unique(test_y)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes,yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.savefig(f'data/Testing data CM Stage {stage_number}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture/Data stage 1: ###\n",
    "**Non-linguistic features:** \n",
    "- All \n",
    "\n",
    "**Linguistic features:** \n",
    "- Polarity (Pos & Neg) scores **with** negation heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">lr 0.554 (0.009)\n",
      ">knn 0.494 (0.013)\n",
      ">cart 0.453 (0.010)\n",
      ">svm 0.556 (0.011)\n",
      ">bayes 0.422 (0.050)\n",
      ">stacking 0.561 (0.011)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVuklEQVR4nO3dfZBddX3H8fcnCaQQIe66EQuhJKVYJ4gyuGRk0AqtMAQEpDIC1sG0Y9M4xY4zQuWPVAKO9YlOfQC9Rob6yEQaSUIRE9AB8aHU7IZsSBBkJ2IJccgNbHkwkWxyv/3jnsXL5u7u2bt7n37385q5k3PPwz2/X87Zz/3d3/ndcxURmJlZumY0uwBmZlZfDnozs8Q56M3MEuegNzNLnIPezCxxs5pdgGp6enpiwYIFzS6GmVnb6O/v3xMR86ota8mgX7BgAX19fc0uhplZ25D0m7GWuevGzCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLXEt+YcrMrF1Iqmm7Rv4WSMcEfTscDOtMtZ6b4POzFYx1DCS1zPHpmKBvh4NhnWm888/np00H99GbmSXOQW9mljgHvZlZ4hz0ZmYT6O7uRtKkHsCkt5FEd3f3tJe/Yy7GmpnVamhoqGEXxacyCmssbtGbmSXOQW9mljgHvVkD1NLH22r9vJZfcW+RpRuWsmffnmYXBXDQmzXESB9vox5DQ0PNrnJHK2wtsPnpzRQGCs0uCgBqxW/d9fb2RqN+HNzfPLRGaPR55vN6mq2cm3vV4swZLJl/LC/NmMHsUokNO3fRc7A0yf09N8kCgqT+iOittsyjbszMJqDrn8/9xll48BOUHl8LpWFKs2ZTOOejrHjrivz7koiVNRZ0DEl13bT7WFezSq3Wz2sTK+4tsn5wPcOlYQCGS8OsG1zX9GOYVIu+3ce6WrriuqMn9fEfoPCaLjYf9SoKt/Sy4pnJ9bnHdUdPan2bHoWtBUrxym6aUpQoDBQm1aqfbkkF/WQV9xa55oFruPEdN9JzRE+zi2MJm8xHf8hahncsIQ6+xLquHpZ/sG9S52g9Pv7bxAZ2D7zcmh8xXBpmy+4tzSlQpqODvvLKeDPfbc1Gq2wZtkKL0PJZc9GaZhehqrRG3bTBlXHrTJMZBVPcW2TJHUt46eBLL8+bPXM2G96zIXer3qNuplcju2q7urp49tlnJ73dlEfdSDoP+AIwE7glIj49avlZwHrg19msOyLihmzZE8ALwEHgwFgFmQ7tfmXcDFq3n7eT1fKm2UpvthOOupE0E7gZWAIsAq6QtKjKqj+JiFOzxw2jlp2dza9byE9Gq14ZN4PW7ee19pWnRb8YGIyIHQCSVgMXA4/Us2D15BaTtbJW7ee19pVnHP1xwJMVz3dm80Y7Q9KApB9IOrlifgD3SOqXtGysnUhaJqlPUl+xWMxV+Fq5xWRmnSRPi77aVYjRHU+bgRMi4kVJ5wPrgJOyZWdGxC5JrwXulfRoRDxwyAtGrAJWQflibN4KHFLYKVw02cY2VLW6h+rq6qp5P9aZGn1Bz2xEnqDfCRxf8Xw+sKtyhYh4vmL6bklfltQTEXsiYlc2f7ektZS7gg4J+unQ7hdMLF21nmM+P2065Om62QScJGmhpMOBy4E7K1eQ9DplzRVJi7PXfUbSHElHZfPnAOcC26azAmZmNr4JW/QRcUDSVcBGysMrb42I7ZKWZ8sLwKXAhyQdAPYBl0dESDoGWJu9B8wCbouIDXWqi5mZVZHWF6Zq4I/G1sp8fravJtyaeswvTCV190ozMzuUg97MLHEOejOzxDnozcwS1zG3KR7vyyrjLfOFMDNrdx0T9A5sM6uHdmhEdkzQm5nVQzs0It1Hb2aWOAe9mVniHPRmZolzH71Zk010++JWuaBn7ctBb9ZkDmurN3fdmJklzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY43wIhARPdK2U8/vq9Wfoc9AkYL6wlOcytqdwQaT4HvZnVlRsizec+ejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56BvE93d3Uia9AOoabvu7u4m19jMpkuuoJd0nqTHJA1KurbK8rMkPSdpS/b4eN5tLZ+hoSEiomGPoaGhZlfZzKbJhF+YkjQTuBk4B9gJbJJ0Z0Q8MmrVn0TEu2rc1szM6iRPi34xMBgROyJiP7AauDjn609lWzMzmwZ5gv444MmK5zuzeaOdIWlA0g8knTzJbZG0TFKfpL5isZijWGZmlkeeoK92R6LRN6fYDJwQEW8GvgSsm8S25ZkRqyKiNyJ6582bl6NYZmaWR56g3wkcX/F8PrCrcoWIeD4iXsym7wYOk9STZ1urn+LeIks3LGXPvj3NLoqZNVGeoN8EnCRpoaTDgcuBOytXkPQ6ZWP5JC3OXveZPNta/RS2Ftj89GYKA4VmF8XMmmjCoI+IA8BVwEbgl8DtEbFd0nJJy7PVLgW2SRoAvghcHmVVt61HReyVinuLrB9cTxCsG1znVr1ZB8t1P/qsO+buUfMKFdM3ATfl3dbqr7C1QClKAJSiRGGgwIq3rmhyqcysGfzN2ASNtOaHS8MADJeG3ao362AO+gRVtuZHjLTqzazz+KcE20RcdzSsnJtr3YFjX8fw7MNfMW+4NMyWrd+CDZ/Lvz8zS4KDvk3o+udz/7bmmunYn0SsnIYXMrOmc9eNmVniHPRmZolz0JuZJc599G1k5IdEGqGrq6th+zKz+nLQt4m8F2JHk1TztmaWBnfdmJklzkFvZpY4B72ZWeIc9GZmiXPQm5klzqNurOVNZVipRxyZOeitDYwX1h4+ajYxd92YmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4jyOPgETfaFovOUeg26WPgd9AhzWZjYed92YmSXOQW9mljgHvZlZ4hz01hK6u7uRNOkHUNN23d3dTa6xWeP4Yqy1hKGhoYZeVJ7KrY/tUN3d3QwNDdW0bS3Hoquri2effbam/XUit+itbRX3Flm6YSl79u1pdlE63sgbdaMetb6pdCoHvbWtwtYCm5/eTGGg0OyimLU0teIY7N7e3ujr62t2MayRVs6d1OrFmTNYMv9YXpoxg9mlEht27qLnYGmS+3xucuvbmBr9AzD+wZlDSeqPiN5qy3L10Us6D/gCMBO4JSI+PcZ6pwMPApdFxJps3hPAC8BB4MBYBbHOpuufn9QfbuHBT1B6fC2UhinNmk3hnI+y4q0r8u9PIlbWUFCzNjRh142kmcDNwBJgEXCFpEVjrPcZYGOVlzk7Ik51yNt0KO4tsn5wPcOlYQCGS8OsG1znvnqzMeTpo18MDEbEjojYD6wGLq6y3oeB7wG7p7F8ZocobC1Qild205Si5L56szHkCfrjgCcrnu/M5r1M0nHAJUC1v7QA7pHUL2nZWDuRtExSn6S+YrGYo1jWqQZ2D7zcmh8xXBpmy+4tzSmQWYvL00dfbZDr6M7UzwMfi4iDVcbEnhkRuyS9FrhX0qMR8cAhLxixClgF5YuxOcplHWrNRWuaXQSztpIn6HcCx1c8nw/sGrVOL7A6C/ke4HxJByJiXUTsAoiI3ZLWUu4KOiTozcysPvIE/SbgJEkLgaeAy4H3Va4QEQtHpiV9HbgrItZJmgPMiIgXsulzgRumq/CWlkZ+W7Wrq6th+zJrtgmDPiIOSLqK8miamcCtEbFd0vJs+XhXwI4B1mZ/wLOA2yJiw9SLbampdUy0x1ObTSzXOPqIuBu4e9S8qgEfEUsrpncAb55C+czMbIp8CwQzs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLXK573Zg100R3tRxvuW94ZuagtzbgsDabGge9mU1ZXHc0rJzb2P1Zbg56M5syXf/8pD95FfcWueaBa7jxHTfSc0TP5PYnESsntUlH88VYM2uKwtYCm5/eTGFgvN8usungoDezhivuLbJ+cD1BsG5wHXv27Wl2kZLmoDezhitsLVCKEgClKLlVX2cOejNrqJHW/HBpGIDh0rBb9XXmoDezhqpszY9wq76+HPRm1lADuwdebs2PGC4Ns2X3luYUqAN4eKWZNdSai9Y0uwgdxy16M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBKXK+glnSfpMUmDkq4dZ73TJR2UdOlktzUzs/qYMOglzQRuBpYAi4ArJC0aY73PABsnu62ZmdVPnhb9YmAwInZExH5gNXBxlfU+DHwP2F3DtmZmVid5gv444MmK5zuzeS+TdBxwCTD6htITblvxGssk9UnqKxaLOYplZmZ55Al6VZk3+ufePw98LCIO1rBteWbEqojojYjeefPm5SiWmZnlked+9DuB4yuezwd2jVqnF1gtCaAHOF/SgZzbmplZHeUJ+k3ASZIWAk8BlwPvq1whIhaOTEv6OnBXRKyTNGuibc3MrL4mDPqIOCDpKsqjaWYCt0bEdknLs+Vj/tDjWNtOT9HNzCwPRVTtMm+q3t7e6Ovra3YxzCwnSTQySxq9v3YgqT8ieqst8zdjzcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0tcnh8eMTObUPYLcw3R1dXVsH2lwEFvZlNW673hfV/5xnDXjZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mljgHvZlZ4hz0ZmaJc9CbmSXOQW9mlrhcQS/pPEmPSRqUdG2V5RdL2ippi6Q+SW+rWPaEpIdHlk1n4c3MbGIT/sKUpJnAzcA5wE5gk6Q7I+KRitV+BNwZESHpTcDtwBsqlp8dEXumsdxmZpZTnhb9YmAwInZExH5gNXBx5QoR8WL84ffA5gD+bTAzsxaRJ+iPA56seL4zm/cKki6R9CjwfeDvKhYFcI+kfknLplJYMzObvDxBX+2n3Q9psUfE2oh4A/Bu4BMVi86MiNOAJcA/SvqLqjuRlmX9+33FYjFHsczMLI88Qb8TOL7i+Xxg11grR8QDwImSerLnu7J/dwNrKXcFVdtuVUT0RkTvvHnzchbfzMwmkifoNwEnSVoo6XDgcuDOyhUk/ZkkZdOnAYcDz0iaI+mobP4c4Fxg23RWwMzMxjfhqJuIOCDpKmAjMBO4NSK2S1qeLS8A7wGulDQM7AMuy0bgHAOszd4DZgG3RcSGOtXFzMyq0B8Gy7SO3t7e6OvzkHuz1EmiFTOoHUnqj4jeasv8zVgzs8Q56M3MEuegNzNLnIPezCxxDnozs8RNOLzSzGwqsuHVNS33iJzp4aA3s7pyWDefu27MzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEteT96CUVgd80aHc9wJ4G7asZXL/25vq1r0bX7YSIqPo7rC0Z9I0kqW+sm/WnwPVrb65f+2qlurnrxswscQ56M7PEOehhVbMLUGeuX3tz/dpXy9St4/vozcxS5xa9mVniHPRmZonr2KCX9GKzyzAdJC2QtK3Z5Wg1kk6VdH6zy9EpUj0PJX1E0pE1brtU0k1V5i+XdOXUS5dfxwZ9NZJmNrsMNnWSZgGnAg56m6qPADUF/VgiohAR35zO15xIxwe9pLMk3SfpNuDhZpdnKiT9qaSHJF0j6Q5JGyQ9LumzFeu8KOmTkgYkPSjpmGaWeSKSrpS0NSvvtyRdKOl/snr+cKT8klZKWiXpHuCbwA3AZZK2SLqsqZWoQtIcSd/P6rVN0gck3V6x/CxJ/5VNvyjpM5L6szovlnS/pB2SLmpeLQ4xS9I3suO1RtKRkj4uaVNWx1UqO1HS5pGNJJ0kqT+bfoukH2d13Sjpj7P5/yTpkey1V9ej8FWOyXXAscB9ku7L1vmKpD5J2yVdX7Ht6ZJ+nm37C0lHjXrtCyT9t6Se7Fy9Opt/f3ZsfyHpV5Lens0/UtLtWX2/m53ztX/5KiI68gG8mP17FvA7YGGzy1RjPRYA24A/Bx6i3JJdCuwA5gJ/RPl2Esdn6wdwYTb9WWBFs+swTt1OBh4DerLn3UAXfxgt9kHg37LplUA/cET2fClwU7PrME7d3gN8reL5XOB/gTnZ868A7684Zkuy6bXAPcBhwJuBLc2uS8V5GMCZ2fNbgauB7op1vlVx7t0HnJpN/yvw4axOPwfmZfMvA27NpncBs7PpVzfwmDwxcv6NnIPZvzOB+4E3AYdnf2+nZ8uOpvx73EuBm4BLgJ8AXRXn6tXZ9P0V5/D5wA+z6auBr2bTbwQOAL211q3jW/SZX0TEr5tdiCmYB6ynHAxbsnk/iojnIuL3wCPACdn8/cBd2XQ/5T/QVvWXwJqI2AMQEc8C84GNkh4GrqH8ZjDizojY1/hi1uRh4J1Za+7tEfEcsAG4MOt6uoDyMYXyMdtQsd2PI2I4m17Q2GKP68mI+Fk2/W3gbcDZWWv0YcrHc+R43QL8bdZdehlwG+XGyhuBeyVtAVZQPt4AW4HvSHo/5dCrh2rHZLT3Zp9GHsrqsigr928jYhNARDwfESNlPBv4GHBBRAyNsd87sn8r/x7fBqzOXm8b5frXzEFf9rtmF2CKngOeBM6smPdSxfRByi0MgOHImgmj5rciUW4lVvoS5Zb6KcA/UP7EMqJtjmNE/Ap4C+Vw+ZSkjwPfBd5LORA3RcQL2eqVx6xEdmwjokRrHb/RxyqALwOXZsfra/zheH0PWAK8C+iPiGcoH+/tEXFq9jglIs7N1r8AuJny/1l/9mY4vYWvfkxeJmkh5Zb2X0XEm4DvZ/Wpdp6O2AEcBbx+nF2P/K1W/j2qljqMxUGfhv3Au4ErJb2vyWWZTj+i3IJ6DYCkbsofp5/Kln9gnG1foPwH1pIkHQvsjYhvAzcCp1H+GH8a8PeUQ7/d/ImkM7LpK4CfZtN7JL0KuHRkxeyT5kbKXVT/kc1+DJg38hqSDpN0sqQZlLse7wP+GXg18KrpLvwYx6TyPDqacmPiueza0JJs/qPAsZJOz17nqIo3ot8Afw18U1Llp8+J/JTymz6SFgGn1FwxWqs1YFMQEb+T9C7gXsofm9teRGyX9Engx5IOUv64vBL4T0lPAQ8CC8fY/D7g2qwL4FMR0WrBeQrwOUklYBj4UEQclHQX5b7d8d7EWtUvgQ9I+irwOOUQ76LcQn4C2DRq/e9QDsF7ACJiv6RLgS9Kmks5nz4P/Ar4djZPwL9HxP/VofyHHBPgDOAHkn4bEWdLegjYTrml/rOKcl8GfEnSEcA+4J0jLxoRj0n6G8rn7YU5y/Jl4BuStlI+77dS/uReE98CwcyaIht5Mjci/qXZZWk12bWLwyLi95JOpPzp9vURsb+W13OL3swaTtJa4ETK1yPsUEdSHtZ5GOVPMR+qNeTBLXozs+T5YqyZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeL+H83VrYQYlELCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "METRICS FOR TESTING DATA\n",
      "Accuracy: 0.56\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.67      0.62      1369\n",
      "           0       0.43      0.21      0.28      1047\n",
      "           1       0.58      0.71      0.64      1370\n",
      "\n",
      "    accuracy                           0.56      3786\n",
      "   macro avg       0.53      0.53      0.51      3786\n",
      "weighted avg       0.54      0.56      0.53      3786\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeT0lEQVR4nO3cd3gVZd7G8e8vCUECgrTQe0cQpbmiIKKsgLq4CiJFVyyAINgQFBVYC6suoq7AqyiyurqgKDZQXEVZZEUpFkSUprRACqFKAklOnvePHGJQWog5Q3juz3XlImfmOTP3cOA+c56ZxJxziIjIyS8q6AAiIhIZKnwREU+o8EVEPKHCFxHxhApfRMQTKnwREU+o8OWkYGYlzOxdM9tlZjMLsJ2+Zvaf3zNbUMysvZmtCjqHnDhM9+FLJJlZH+AOoDGwB/gaeNg5t7CA270GGAq0c85lFTTnic7MHNDAObc26CxSdOgMXyLGzO4AngTGAZWAmsBkoPvvsPlawGofyv5YmFlM0BnkxKPCl4gwszLAA8AQ59ws59xe51ymc+5d59xd4THFzexJM9sS/nrSzIqH13U0s81mdqeZJZvZVjPrH173V2A00MvMfjazG8xsrJm9nGf/tc3MHShCM7vOzH40sz1m9pOZ9c2zfGGe57UzsyXhqaIlZtYuz7r5Zvagmf0vvJ3/mFmFwxz/gfwj8uS/3My6mdlqM9tuZqPyjG9rZovMbGd47EQziw2vWxAe9k34eHvl2f5IM0sEph1YFn5OvfA+WoYfVzWzbWbWsSCvqxQtKnyJlHOAU4A3jzDmXuAPwJlAC6AtcF+e9ZWBMkA14AZgkpmVdc6NIedTw6vOuVLOualHCmJmJYF/AF2dc6cC7ciZWvr1uHLAnPDY8sAEYI6Zlc8zrA/QH4gHYoHhR9h1ZXL+DqqR8wb1HNAPaAW0B0abWd3w2BBwO1CBnL+7C4HBAM65DuExLcLH+2qe7Zcj59POgLw7ds6tA0YCr5hZHDAN+Kdzbv4R8spJRoUvkVIe2HaUKZe+wAPOuWTnXArwV+CaPOszw+sznXPvAT8DjY4zTzbQzMxKOOe2Oue+O8SYS4A1zrl/OeeynHPTgR+Ay/KMmeacW+2cSwdeI+fN6nAyyblekQnMIKfMn3LO7Qnv/zvgDADn3DLn3Ofh/a4HngXOP4ZjGuOc2x/OcxDn3HPAGuALoAo5b7DiERW+REoqUOEoc8tVgQ15Hm8IL8vdxq/eMNKAUvkN4pzbC/QCBgFbzWyOmTU+hjwHMlXL8zgxH3lSnXOh8PcHCjkpz/r0A883s4ZmNtvMEs1sNzmfYA45XZRHinNu31HGPAc0A552zu0/ylg5yajwJVIWAfuAy48wZgs50xEH1AwvOx57gbg8jyvnXemc+8A515mcM90fyCnCo+U5kCnhODPlx/+Rk6uBc640MAqwozzniLfcmVkpci6aTwXGhqesxCMqfIkI59wucuatJ4UvVsaZWTEz62pmj4WHTQfuM7OK4Yufo4GXD7fNo/ga6GBmNcMXjO85sMLMKpnZn8Jz+fvJmRoKHWIb7wENzayPmcWYWS+gKTD7ODPlx6nAbuDn8KePm3+1Pgmo+5tnHdlTwDLn3I3kXJt4psAppUhR4UvEOOcmkHMP/n1ACrAJuAV4KzzkIWApsBz4FvgyvOx49vUh8Gp4W8s4uKSjgDvJOYPfTs7c+OBDbCMVuDQ8NhUYAVzqnNt2PJnyaTg5F4T3kPPp49VfrR8LvBi+i+eqo23MzLoDXciZxoKc16HlgbuTxA/6wSsREU/oDF9ExBMqfBERT6jwRUQ8ocIXEfHECfsLlkp0GKuryUVU6kdjgo4gBbAueW/QEaQAmlcvddif19AZvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHgiJugAJ6shPc6m/6WtMINps79k4szPGXdzZ7q1a0RGVoifErYz4JG32fXzPlo3qcbE4ZcBYAYPT5vPO5/+EPAR+Gvs/aNYsGA+5cqV5/U33wXgmclPM+uNmZQtWw6AW4bdTvsO5wMw9flneXvWG0RFRzHi7ntpd277wLL7bltyIk8/MpqdO1Ixi6LzJX/mkiv7sH7daqY8MY59+9KoWKkqt456iLiSpQBYv24NU554mLS0vURFGY9M/hexscUDPpLCYc65wt+JWWNgGtASuNc5N/5ozynRYWzhByskTevE89KYHrQf+BwZWSHe+Xs/hk2YTe0qZZn/5U+EQtk8NOgiAO575iNKFC9GRlaIUCibyuVL8cULN1P3iscJhbIDPpLjk/rRmKAjFMiypUuIi4vj/nvvPqjw4+LiuPa6Gw4au27dWu4ZcScvT59JSnIyg27qz1uz5xIdHR1E9N/FuuS9QUc4bjtSU9iRuo26DZuQnraXEYP6MeKBx5n42BiuHXgbp7doxbz33yY5MYHe/QcTCmVx18C+DLvnQWrXa8ieXTuJK3VqkX79mlcvZYdbF6kpne3AMOCoRX8yaFyrAotXbiZ9fyahUDaffr2e7u2bMG/JutwSX/zdZqpVLA2QOw6geGwMkXgTlsNr1boNZcqUOaax8z+Zx8VduxEbG0u16tWpUbMmK75dXsgJ5XDKlq9I3YZNACgRV5JqteqwfVsyWzZtoOkZLQFo0epsvljwMQDfLP2cWnUbULteQwBOLXNakS77o4lI4Tvnkp1zS4DMSOwvaN/9lMx5LWpRrnQJShQvRpc/NKB6fOmDxlzb7Sw++Hxt7uM2Taqx7MXBLJ02mGGPzy6yZ/cnsxnTX+GqK/7E2PtHsXvXLgBSkpKoXKlK7pj4SpVJTk4KKqLkkZy4hfVrf6BBk2bUqF2PJZ/9F4BF//2IbSk5r9GWzRsxMx4cOYS7BvbhrRkvBhm50OmibSFYtWEbj/97IbMnXMs74/uxfF0SWXkKfMQ17QmFspnx4S9ngku+T6DVXyZz3sAp3NWvPcVjdXnlRNLzqt68+96HzHj9LSpUrMiE8Y8CcKgPY2aH/UQtEZKensb4sXdx3eDhxJUsxZC7RjP37dcYMagv6elpxMQUAyAUyuKHFV9z66iHeOipqSxe+AnLv1wccPrCc0IVvpkNMLOlZrY0a+uyoOMUyItzvqLdjc/Seeg0duxOZ+3m7QD07dKCbuc05LoHZx3yeas2bGNvegan14mPZFw5ivIVKhAdHU1UVBRXXNmTFSu+BSC+ciUSk7bmjktOSqRiRb12QcrKymT82Ltof2FX/tC+EwDVatZh9GOTeeyZVzjvgoupXLU6AOUrVKLpGS0pXaYsxU8pwVlnn8tPa07eGyYKrfDNbIiZfR3+qnosz3HOTXHOtXbOtY6p0qqwokVExdNKAlAjvgzdOzThtY++pXPb+tzZ5zx63DOd9P2/zG7VqnIa0dE5L0XNSmVoWLMCGxJ3BhFbDiMlJTn3+4/nfUS9+g0A6NixEx+8/x4ZGRkkbN7Mxg0baNb8jKBies85x+TxD1K9Zh0u69kvd/muHTknXNnZ2bz+ylQ6X3YlAGe2OYcNP65h/750QqEsVi7/kuq16gSSPRIKbd7AOTcJmFRY2z/RTX/wKsqViSMzK8RtT8xh58/7eOK2bhSPjWb2hGsBWLxyM8Men0275jUZ3vc8MrOyyXaOWyfMIXVXWsBH4K+7R9zBsiVL2LlzBxdfeD6Dhgxl2ZLFrPrhe8yMKtWqcd/ovwJQr34D/nhxV67sfgnRMdHcfe/ok/qi34nuhxVfs+DDOdSsU5/hA3oD0OeGIWzdvJG5b88E4Oz2F9Cpy58AKHVqaS7r0Y+Rg6/FzGjZ9lxa/eHkva02UrdlVgaWAqWBbOBnoKlzbvfhnlOUb8v0XVG/LdN3Rfm2TDnybZkRuTLonEsEqkdiXyIicmgn1EVbEREpPCp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPqPBFRDyhwhcR8YQKX0TEEyp8ERFPxAQd4HBeeOqmoCPIcdq+NzPoCFIAG3emBR1BCqB59VKHXaczfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfHEUQvfcvQzs9HhxzXNrG3hRxMRkd/TsZzhTwbOAXqHH+8BJhVaIhERKRQxxzDmbOdcSzP7CsA5t8PMYgs5l4iI/M6O5Qw/08yiAQdgZhWB7EJNJSIiv7tjKfx/AG8C8Wb2MLAQGFeoqURE5Hd31Ckd59wrZrYMuBAw4HLn3PeFnuwkkJ0dYsqomzm1bAX6jhzHx6++wA/LPsPMKFn6NC6/eSSly1UA4NO3/s2Xn7xHVFQUXa8bSv0WbQJO76/kpEQe+esodqRuw6KiuOTyHlzZqx/PPv04ixbOJyamGFWr12DEfQ9S6tTSLP3iM56f/CRZWZnExBRj4NA7Oav12UEfhpcyM/Yz8f6hZGVmkB0K0eKcjnS5+gYS1q/l9WfHs39fOuUqVqbfbaM5Ja4k25O38sit/YivWhOAWg1Pp+fA4QEfReEx59yRB5jVPNRy59zGfO3IrAvwFBANPO+ce+RI46d/lXDkYEXAZ3NmsmXdKvanp9F35Dj2pe3llLiSAHz+/ixSEjZw2Y23k7x5PW/84yFuengye3ak8tJDwxn65EtERUUHfATHp33tikFHKJDUbSmkbkuhYeOmpO3dy6DrevHAY0+xLTmJs1q1JTomhikTJwAw4JY7WLPqe8qWK0+FivH8tG4NI28bxGvvzgv4KI7fNwk7g45w3JxzZOxLp3iJOEJZWTx932Auv/5W3nz+SS77y2Dqn34WX8ybw/bkrXTtfSPbk7fy/LiRjHjypaCj/24uaRZvh1t3LFM6c4DZ4T/nAT8C7+cnQPgawCSgK9AU6G1mTfOzjaJmV2oKa778nJaduuUuO1D2AJn793HgVVm19DOatetETLFYysZXoVzlaiSs/SHCieWA8hUq0rBxzj/PuJIlqVW7DtuSk2h9djuiY3I+FDdt1oJtyUkANGjUhAoV4wGoXbc+Gfv3k5GREUx4z5kZxUvEARAKZRHKysKA5C0bqdf0TAAatmjN8s/nB5YxSMcypdM872MzawkMzOd+2gJrnXM/hrcxA+gOrMzndoqMuS9OonPfgexPTzto+bwZU/lmwX8oHleS60bnnCXu3p5C9Qa/vP+VLleR3du3RTSvHFrilgTWrv6BJs3OOGj5++++SceLLv7N+AWffEiDho2JjdWNbEHJDoWYMOJGtiUmcG6XP1Or4elUqVmX75YspFnb9nzz2Sfs3JacO3578lYeH349xUvE0a33TdRt2iLA9IUr3z9p65z7EsjvBHM1YFOex5vDyw5iZgPMbKmZLZ33xsv5jXbCWLVsESXLnEbVug1/s+7Cq2/gjsmvcsZ5F7H4g7cOuw2zw34qkwhJT0tj7D23M/i2kZQsWSp3+SvTphAdE81FXS49aPz6H9fy3KQnuP3uMZGOKnlERUcz/PFpjJnyBhvXfM/WjT/Sa/DdLJz7JhPuuoH9+9KJjikGQOmy5bn/2de5c/wLdL9uKC8/+QD70vYGfASF56hn+GZ2R56HUUBLICWf+zlUe/1mjt45NwWYAkV7Dn/T6hWsWvYZa776gqzMDPanp/HGxHFcecuo3DHNz+3EK4+O4oKe1+Wc0af+8le6e3sKp5YtH0R0CcvKymTsPbdz4cWX0P6Ci3KXfzDnbRb977+Mn/j8QW/KKcmJjB55G3ePHkfV6jWCiCy/UqLkqdRvdhY/fPUFF3TvzaDwJ+rkLRtZuWwRADHFYokplvNprEa9RpSvXJWULZuoUb9xYLkL07Gc4Z+a56s4OXP53fO5n81A3v8F1YEt+dxGkXFR75u4c/Jr3D5xOj2G3U+d08/iyltGkbp1c+6YVcs+o0L4zoBGrc5hxWcfk5WZwY7kraQmJlDtJP0HVxQ45xj/8Bhq1q5Lzz5/yV2+eNFCZvzrBR76+9OcckqJ3OU/79nNqDuGcOPNt9KsxVlBRJawn3ftIH3vHgAy9u9n9fKlxFeryZ5dOwDIzs7mo9dfot0fu+eOzw6FAEhN3ELK1s2Uq1Q1mPARcMQz/PDF1lLOubsKuJ8lQAMzqwMkAFcDfQq4zSLno+nPsW3LJiwqitMqxHPpjbcDEF+jDqef05FJd/YnKjqaS/oPK7J36JwMVnzzFR++/y516jVgwDU9ALjh5mFMnPAImRkZjBg2AIAmzc7g9pGjeWvmdLZs3sTL057l5WnPAvDoU89Stpw+pUXa7h2pTJ84juxQCOccLdpdwOmtz2XB7Jn8b+4sAJqffT5twzdTrFv5DXNnTCUqOpqoqCh6DhhOyVNLB3kIheqwt2WaWYxzLsvM5jnnLizwjsy6AU+Sc1vmC865h480vihP6fiuqN+W6buifFumHPm2zCOd4S8mZ77+azN7B5gJ5F7NcM7Nyk8I59x7wHv5eY6IiPx+juWXp5UDUoFO5FxotfCf+Sp8EREJ1pEKPz58h84Kfin6AzTdIiJSxByp8KOBUhzjLZUiInJiO1Lhb3XOPRCxJCIiUqiOdB++ftRTROQkcqTCL/CtmCIicuI4bOE757ZHMoiIiBSufP/yNBERKZpU+CIinlDhi4h4QoUvIuIJFb6IiCdU+CIinlDhi4h4QoUvIuIJFb6IiCdU+CIinlDhi4h4QoUvIuIJFb6IiCdU+CIinlDhi4h4QoUvIuIJFb6IiCdU+CIinlDhi4h4QoUvIuIJFb6IiCdU+CIinlDhi4h4wpxzQWc4pNVJaSdmMDmqmuXjgo4gBVC2zS1BR5ACSP9qoh1unc7wRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMqfBERT6jwRUQ8ocIXEfGECl9ExBMxQQc4GaUkJfLEuPvZkZqKRRldLruSP/XsA8C7b0xnzqxXiYqOps057el/822sXrmCieMfBMA5R5/+gzinQ6cgD8Fro++7hwX/nU+5cuWZ9fbsg9a9OG0qE8Y/xvyFiyhbthwAU597ljffeJ2o6ChG3nMf557XPojYEjakd0f6X9EOM2ParP8x8d/zAbj56vMZ1KsDWaFs5n66gnufepuru7bmtr9clPvc5g2qck7vR1m+OiGg9IUrYoVvZi8AlwLJzrlmkdpvEKKjo7l+8B3Ub9SEtLS93H5jH85sczY7t2/ni4XzeXraaxSLjWXnju0A1KxbjyemvEJ0TAzbt6Uw7PpetG3XgegYvR8HofvlV9C7Tz/uvWfkQcsTt25l0WefUaVK1dxl69auZe57c5j1zhySk5MYeGN/3pnzAdHR0ZGOLUDTelXof0U72l/zdzIyQ7wzaTDvL/yOavGncWnH5rS56m9kZGZRsWwpAGa8v5QZ7y8F4PT6VZn5xICTtuwhslM6/wS6RHB/gSlXoSL1GzUBIC6uJDVq1SE1JYX33p5Jj779KRYbC8Bp4TPEU04pkVvuGRkZmFkwwQWAVq3bULpMmd8s//ujf+P2O+866PWZ/8k8unS7hNjYWKpXr0GNGrVY8e3ySMaVPBrXqczib9eTvi+TUCibT5etpfsFLRjQsz3jp31IRmYWACk7fv7Nc6/q0orX5i6LdOSIiljhO+cWANsjtb8TRdLWLaxbs4pGTZuxZdMGvlv+FXcOvIa7h97A6u+/yx23auW3DL72Sob278ngO+/V2f0JZv7H84ivFE+jxo0PWp6UlESlypVzH1eqXInkpKRIx5Ow79Zt4byW9SlXpiQlTilGl/NOp3rlstSvFc+5Z9VjwUvD+c/zt9Kqac3fPLfHH1vy2tylAaSOHLVKIUpPS+Nv9w/npqHDiStZilAoxM97djP+mZdY8/13PDpmBM+/Ohszo1HT5kx+6Q02rf+RJ8aNptXZ5xJbvHjQhyBAeno6z015hmeee+G3K537zSJ9QgvOqp+SePyfHzL7/25hb/p+lq9OICsrREx0FGVLx9Hh2vG0Pr0WLz92PU0uHZv7vDbNapG2L5OV67YGFz4CTqi7dMxsgJktNbOlr/7rEP+5ipCsrEz+dv9wOnbuSrvzLwSgQsVKtOtwIWZGw6bNiIqKYveuHQc9r0btupxSogQbflobRGw5hM2bNpKQsJmrruhO186dSEpK5OoeV7AtJYVKlSuTlJiYOzYpMYmK8fEBppUX31pEuz6P0vmGJ9mxay9rN6aQkLSTt+Z9A8DS7zaQne2oEJ7HB+h5cauT/uweTrDCd85Ncc61ds617nXN9UHHOW7OOf7x6F+pUasOl/e6Jnf5H9p35JsvFwOQsGkDWZmZlC5TlsQtCYSycuYWkxO3kLBxPfGVqx5y2xJ5DRo2Yv6ni3j/w495/8OPqVSpMjNen0WFihU5/4JOzH1vDhkZGWzevImNG9fTrPkZQUf22oELsjUql6V7pxa8Nncp785fTse2DQGoXzOe2GIxbAvP45sZV3Q+i5kfnNzz96ApnUKx8tuv+eSDOdSu24Bh1/cC4NqbbuGibpfzj0fGMuQvPYiJKcZtox7AzFj57Ve8/so0YmJiMIti0B2jKHNa2YCPwl8jh9/B0iWL2blzB507deDmIUO54sqehxxbv34D/tilK3/+Uzeio6MZdd9o3aETsOnjb6TcaSXJzApx2yOvsXNPOi++tYhnx/Zl6cxRZGSGuHH0v3LHn9eyPglJO1mfkBpg6sgwd4g5yELZkdl0oCNQAUgCxjjnph5u/OqktMgEk99dzfJxQUeQAijb5pagI0gBpH818bAXkSJ2hu+c6x2pfYmIyG+dUHP4IiJSeFT4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHhChS8i4gkVvoiIJ1T4IiKeUOGLiHjCnHNBZ/CSmQ1wzk0JOoccH71+RZfPr53O8IMzIOgAUiB6/Youb187Fb6IiCdU+CIinlDhB8fLOcSTiF6/osvb104XbUVEPKEzfBERT6jwRUQ8ocKPMDNrbGaLzGy/mQ0POo/kj5l1MbNVZrbWzO4OOo8cOzN7wcySzWxF0FmCosKPvO3AMGB80EEkf8wsGpgEdAWaAr3NrGmwqSQf/gl0CTpEkFT4EeacS3bOLQEyg84i+dYWWOuc+9E5lwHMALoHnEmOkXNuATknXN5S4Yscu2rApjyPN4eXiRQJKnyRY2eHWKb7mqXIUOFHgJkNMbOvw19Vg84jx20zUCPP4+rAloCyiOSbCj8CnHOTnHNnhr9UEEXXEqCBmdUxs1jgauCdgDOJHDP9pG2EmVllYClQGsgGfgaaOud2BxpMjomZdQOeBKKBF5xzDwebSI6VmU0HOgIVgCRgjHNuaqChIkyFLyLiCU3piIh4QoUvIuIJFb6IiCdU+CIinlDhi4h4QoUvEmZmofAPx60ws5lmFleAbf3TzHr8nvlECkqFL/KL9PAPxzUDMoBBeVeGf1umSJGlwhc5tE+B+mbW0cw+MbN/A9+aWbSZ/d3MlpjZcjMbCGA5JprZSjObA8QHml7kEGKCDiByojGzGHJ+5/3c8KK2QDPn3E9mNgDY5ZxrY2bFgf+Z2X+As4BGQHOgErASeCHy6UUOT4Uv8osSZvZ1+PtPgalAO2Cxc+6n8PI/AmfkmZ8vAzQAOgDTnXMhYIuZfRy52CLHRoUv8ot059yZeReYGcDevIuAoc65D341rhv6VclygtMcvkj+fADcbGbFAMysoZmVBBYAV4fn+KsAFwQZUuRQdIYvkj/PA7WBLy3n9D8FuBx4E+gEfAusBv4bUD6Rw9JvyxQR8YSmdEREPKHCFxHxhApfRMQTKnwREU+o8EVEPKHCFxHxhApfRMQT/w87P9JU5jyZHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_X_stage1 = train_X.iloc[:, 3:-3]\n",
    "test_X_stage1 = test_X.iloc[:, 3:-3]\n",
    "\n",
    "models = run_final_model(train_X_stage1.values, train_y.values.astype('int'), stage_number=1)\n",
    "models['stacking'].fit(train_X_stage1.values, train_y.values.astype('int'))\n",
    "predictions_stage1 = models['stacking'].predict(test_X_stage1.values)\n",
    "test_data_metrics(test_y.values.astype('int'), predictions_stage1, stage_number=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name, file_format = '.pickle'):\n",
    "    print('Loading file:', file_name, file_format, '...')\n",
    "    file_path = 'data/' + file_name + file_format\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        file = pickle.load(handle)\n",
    "    return file\n",
    "models = {}\n",
    "models['stacking'] = load_file(str('models/Stacking'),file_format = '.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture/Data Stage 2: ###\n",
    "**Non-linguistic features:** \n",
    "- All\n",
    "\n",
    "**Linguistic features:** \n",
    "- Reports TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_stage2 = train_X.iloc[:, 4:-4]\n",
    "#append the last column which was ignored on the previous column selection (as it was the last column and it went up to the last 4th)\n",
    "#train_X_stage2['tfidf pred. proba'] = train_X['tfidf pred. proba'].values\n",
    "#train_X_stage2\n",
    "\n",
    "#test_X_stage2 = test_X.iloc[:, 4:-4]\n",
    "#append the last column which was ignored on the previous column selection (as it was the last column and it went up to the last 4th)\n",
    "#test_X_stage2['tfidf pred. proba'] = test_X['tfidf pred. proba'].values\n",
    "#test_X_stage2\n",
    "\n",
    "models = run_final_model(train_X_stage2.values, train_y.values.astype('int'), stage_number=2)\n",
    "models['stacking'].fit(train_X_stage2.values, train_y.values.astype('int'))\n",
    "predictions_stage2 = models['stacking'].predict(test_X_stage2.values)\n",
    "test_data_metrics(test_y.values.astype('int'), predictions_stage2, stage_number=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture/Data Stage 3: ###\n",
    "**Non-linguistic features:** \n",
    "- All\n",
    "\n",
    "**Linguistic features:** \n",
    "- Polarity scores **with** negation heuristics.\n",
    "- Reports TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_stage3 = train_X_stage1\n",
    "train_X_stage3['tfidf pred. proba'] = train_X['tfidf pred. proba'].values\n",
    "train_X_stage3\n",
    "\n",
    "test_X_stage3 = test_X_stage1\n",
    "test_X_stage3['tfidf pred. proba'] = test_X['tfidf pred. proba'].values\n",
    "test_X_stage3\n",
    "\n",
    "models = run_final_model(train_X_stage3.values, train_y.values.astype('int'), stage_number=3)\n",
    "models['stacking'].fit(train_X_stage3.values, train_y.values.astype('int'))\n",
    "predictions_stage3 = models['stacking'].predict(test_X_stage3.values)\n",
    "test_data_metrics(test_y.values.astype('int'), predictions_stage3, stage_number=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture/Data Stage 4: ###\n",
    "**Non-linguistic features:** \n",
    "- All\n",
    "\n",
    "**Linguistic features:** \n",
    "- Best performing Stage (reports polarity alone, tfidf probabilities alone, or both combined)\n",
    "- Twitter polarity scores (using Flair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_stage4 = train_X.iloc[:, 3:]\n",
    "test_X_stage4 = test_X.iloc[:, 3:]\n",
    "\n",
    "dropped_idxs = []\n",
    "for i, (index, row) in enumerate(train_X_stage4.iterrows()):\n",
    "    if row.isnull().any():\n",
    "        dropped_idxs.append(i)\n",
    "train_y.drop(train_y.index[dropped_idxs], inplace=True)\n",
    "train_X_stage4.dropna(inplace=True)\n",
    "\n",
    "\n",
    "dropped_idxs = []        \n",
    "for i, (index, row) in enumerate(test_X_stage4.iterrows()):\n",
    "    if row.isnull().any():\n",
    "        dropped_idxs.append(i)\n",
    "test_y.drop(test_y.index[dropped_idxs], inplace=True)\n",
    "test_X_stage4.dropna(inplace=True)\n",
    "\n",
    "models = run_final_model(train_X_stage4.values, train_y.values.astype('int'), stage_number=4)\n",
    "models['stacking'].fit(train_X_stage4.values, train_y.values.astype('int'))\n",
    "predictions_stage4 = models['stacking'].predict(test_X_stage4.values)\n",
    "test_data_metrics(test_y.values.astype('int'), predictions_stage4, stage_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
